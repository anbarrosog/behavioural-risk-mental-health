{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7642363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea88b400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/usuario/anaconda3/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "geslin_data = pd.read_excel('geslinactualitzat.xlsx')\n",
    "geslin_data['Fecha'] = pd.to_datetime(geslin_data['Fecha'], format='%d-%m-%Y', errors='coerce')\n",
    "geslin_data = geslin_data.dropna(subset=['Fecha'])\n",
    "geslin_data = geslin_data[(geslin_data['Fecha'] >= '2024-01-22') & (geslin_data['Fecha'] <= '2025-02-21')]\n",
    "geslin_data = geslin_data.sort_values('Fecha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dad943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Merge Pizarra Files\n",
    "files = ['pizarra.xlsx', 'pizarra1.xlsx', 'pizarra17062102.xlsx']\n",
    "pizarra_data = pd.concat([pd.read_excel(f) for f in files], ignore_index=True)\n",
    "pizarra_data['Data'] = pd.to_datetime(pizarra_data['Data'], dayfirst=True, errors='coerce')\n",
    "pizarra_data = pizarra_data.dropna(subset=['Data'])\n",
    "pizarra_data = pizarra_data[(pizarra_data['Data'] >= '2024-01-22') & (pizarra_data['Data'] <= '2025-02-21')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "154d5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Geslin NHC Columns\n",
    "nhc_columns = ['NHC_AV', 'NHC_AFO', 'NHC_AFP', 'NHC_IA', 'NHC_IS', 'NHC_IF', 'NHC_PF', 'NHC_F']\n",
    "geslin_data[nhc_columns] = geslin_data[nhc_columns].replace(r'^\\s*$', np.nan, regex=True)\n",
    "geslin_filtered = geslin_data[['Fecha', 'Turno'] + nhc_columns].copy()\n",
    "geslin_filtered = geslin_filtered[\n",
    "    geslin_filtered[nhc_columns].apply(lambda row: row.apply(lambda x: isinstance(x, (int, float))).any(), axis=1)\n",
    "]\n",
    "geslin_filtered = geslin_filtered.drop_duplicates(subset=['Fecha', 'Turno'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25527092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize and Restructure\n",
    "structured_rows = []\n",
    "seen = set()\n",
    "for _, row in geslin_filtered.iterrows():\n",
    "    fecha, turno = row['Fecha'], row['Turno']\n",
    "    for col in nhc_columns:\n",
    "        nhc = row[col]\n",
    "        if pd.notna(nhc):\n",
    "            key = (nhc, fecha, turno) \n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                structured_rows.append({\n",
    "                    'NHC': str(int(nhc)), 'Fecha': fecha, 'Turno': turno,\n",
    "                    **{c: int(c == col) for c in nhc_columns}\n",
    "                })\n",
    "\n",
    "nhc_data = pd.DataFrame(structured_rows)\n",
    "nhc_data = nhc_data.sort_values(by=['Fecha', 'NHC']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1f75e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and Assign Turno in Pizarra\n",
    "pizarra_data['Pacient'] = pd.to_numeric(pizarra_data['Pacient'], errors='coerce')\n",
    "pizarra_data = pizarra_data.dropna(subset=['Pacient'])\n",
    "pizarra_data['Pacient'] = pizarra_data['Pacient'].astype(int).astype(str)\n",
    "pizarra_data['Fecha'] = pizarra_data['Data'].dt.date\n",
    "pizarra_data['Hora'] = pizarra_data['Data'].dt.hour\n",
    "\n",
    "def classify_turno(hour):\n",
    "    if pd.isna(hour): return 'Unknown'\n",
    "    if 8 <= hour < 15: return 'M'\n",
    "    elif 15 <= hour < 22: return 'T'\n",
    "    return 'N'\n",
    "\n",
    "pizarra_data['Turno'] = pizarra_data['Hora'].apply(classify_turno)\n",
    "pizarra_data['Fecha'] = pd.to_datetime(pizarra_data['Fecha'])\n",
    "pizarra_clean = pizarra_data.drop(columns=['Hora', 'Data'])\n",
    "pizarra_clean = pizarra_clean.rename(columns={'Pacient': 'NHC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b236b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Turno and Items from Pizarra\n",
    "nhc_data['NHC'] = nhc_data['NHC'].astype(str)\n",
    "pizarra_clean['NHC'] = pizarra_clean['NHC'].astype(str)\n",
    "nhc_data = pd.merge(\n",
    "    nhc_data,\n",
    "    pizarra_clean[['NHC', 'Fecha', 'Turno', 'Ítem', 'Càrrega assistencial']],\n",
    "    on=['NHC', 'Fecha', 'Turno'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9be9e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add patients from pizarra that are not in geslin\n",
    "nhc_in_geslin = set(nhc_data['NHC'].unique())\n",
    "nhc_in_pizarra = set(pizarra_clean['NHC'].unique())\n",
    "missing_nhcs = nhc_in_pizarra - nhc_in_geslin\n",
    "missing_rows = pizarra_clean[pizarra_clean['NHC'].isin(missing_nhcs)].copy()\n",
    "\n",
    "for col in nhc_data.columns:\n",
    "    if col not in missing_rows.columns:\n",
    "        if col in nhc_columns:\n",
    "            missing_rows[col] = 0\n",
    "        elif col in ['Ítem', 'Càrrega assistencial']:\n",
    "            missing_rows[col] = \"\"\n",
    "        else:\n",
    "            missing_rows[col] = np.nan\n",
    "\n",
    "missing_rows = missing_rows[nhc_data.columns]\n",
    "nhc_data = pd.concat([nhc_data, missing_rows], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f1171aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword mapping (supporting strings and lists of synonyms/variants)\n",
    "keywords = {\n",
    "    'VIA': 'autolítiques',\n",
    "    'VIPM': 'passives de mort',\n",
    "    'AMG': [\n",
    "        r'autolesions moderades/greus\\*',\n",
    "        r'autolesions.*moderades',\n",
    "        r'autolesions.*greus',\n",
    "        r'autolesions moderades',\n",
    "        r'autolesions greus',\n",
    "        r'autolesions(?!.*lleus)',\n",
    "        r'autolesivas(?!.*lleus)'\n",
    "    ],\n",
    "    'ALT': 'autolesions lleus',\n",
    "    'CAU': 'conductes autolítiques',\n",
    "    'AOPAH': 'objecte',\n",
    "    'AMED': 'medicació',\n",
    "    'SRAMF': 'família',\n",
    "    'OPCC': 'punts cecs',\n",
    "    'PAP': 'preagitació',\n",
    "    'MVE': 'manifestació verbal',\n",
    "    'HVA': 'hipervigilància',\n",
    "    'DCAT': 'alta durant el torn',\n",
    "    'PIDT': 'dni',\n",
    "    'NPMT': 'medicació',\n",
    "    'AFPI': 'antecedents de fugida',\n",
    "    'DCMSI': 'deteriorament cognitiu',\n",
    "    'PII': 'ingrés involuntari',\n",
    "    'CCA': 'canvi cognitiu',\n",
    "    'SPS': 'suspicàcia',\n",
    "    'CHP': 'heteroagresives cap a persones',\n",
    "    'CHO': 'heteroagresives cap a objectes',\n",
    "    'HVT': 'heteroagresivitat verbal',\n",
    "    'CA': 'amenaçadora',\n",
    "    'DR': 'recent*',\n",
    "    'PH': 'hiperdemandant',\n",
    "    'PD-ABVD': 'abvd',\n",
    "    'PAH': 'higiene',\n",
    "    'PCA': 'ambiental',\n",
    "    'AC': 'contagi',\n",
    "    'PTCA': 'tca',\n",
    "    'PCM': 'mecànica',\n",
    "    'RAC': 'acompanyament constant',\n",
    "    'RAI': 'acompanyament intermitent',\n",
    "    'RCE-PSA': 'somàtica',\n",
    "    'RCO': 'oposicionistes'\n",
    "}\n",
    "\n",
    "# Classify acronyms into source columns\n",
    "item_acronyms = list(keywords.keys())[:25]\n",
    "careload_acronyms = list(keywords.keys())[25:]\n",
    "\n",
    "# Normalize text fields to lowercase\n",
    "nhc_data['Ítem'] = nhc_data['Ítem'].astype(str).str.lower()\n",
    "nhc_data['Càrrega assistencial'] = nhc_data['Càrrega assistencial'].astype(str).str.lower()\n",
    "\n",
    "# Pattern matching loop (handles lists, wildcards, etc.)\n",
    "for acronym, keyword_patterns in keywords.items():\n",
    "    if isinstance(keyword_patterns, str):\n",
    "        keyword_patterns = [keyword_patterns]\n",
    "\n",
    "    regex_parts = []\n",
    "    for pattern in keyword_patterns:\n",
    "        if pattern.endswith('*') and not pattern.endswith(r'\\*'):\n",
    "            # Treat as wildcard\n",
    "            regex_parts.append(r'\\b' + re.escape(pattern[:-1]) + r'\\w*')\n",
    "        else:\n",
    "            # Use as-is (already escaped or raw regex)\n",
    "            regex_parts.append(pattern)\n",
    "\n",
    "    full_pattern = '|'.join(regex_parts)\n",
    "\n",
    "    source_col = 'Ítem' if acronym in item_acronyms else 'Càrrega assistencial'\n",
    "    nhc_data[acronym] = nhc_data[source_col].str.contains(full_pattern, regex=True, case=False, na=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79518898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk Category Aggregation\n",
    "nhc_data['Aggressive'] = nhc_data[['NHC_AV', 'NHC_AFO', 'NHC_AFP']].sum(axis=1)\n",
    "nhc_data['Self-Harm'] = nhc_data[['NHC_IA', 'NHC_IS']].sum(axis=1)\n",
    "nhc_data['Absconding'] = nhc_data[['NHC_IF', 'NHC_PF', 'NHC_F']].sum(axis=1)\n",
    "nhc_data['No_risk'] = (nhc_data[['Aggressive', 'Self-Harm', 'Absconding']].sum(axis=1) == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07926e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RISK CATEGORIES (Total Counts)\n",
      "Aggressive     282\n",
      "Self-Harm       50\n",
      "Absconding      53\n",
      "No_risk       4562\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"RISK CATEGORIES (Total Counts)\")\n",
    "print(nhc_data[['Aggressive', 'Self-Harm', 'Absconding', 'No_risk']].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1ded38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NHC', 'Fecha', 'Turno', 'NHC_AV', 'NHC_AFO', 'NHC_AFP', 'NHC_IA', 'NHC_IS', 'NHC_IF', 'NHC_PF', 'NHC_F', 'Ítem', 'Càrrega assistencial', 'VIA', 'VIPM', 'AMG', 'ALT', 'CAU', 'AOPAH', 'AMED', 'SRAMF', 'OPCC', 'PAP', 'MVE', 'HVA', 'DCAT', 'PIDT', 'NPMT', 'AFPI', 'DCMSI', 'PII', 'CCA', 'SPS', 'CHP', 'CHO', 'HVT', 'CA', 'DR', 'PH', 'PD-ABVD', 'PAH', 'PCA', 'AC', 'PTCA', 'PCM', 'RAC', 'RAI', 'RCE-PSA', 'RCO', 'Aggressive', 'Self-Harm', 'Absconding', 'No_risk']\n"
     ]
    }
   ],
   "source": [
    "print(nhc_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3d7d560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RISK CATEGORIES (Total Counts):\n",
      "Aggressive     282\n",
      "Self-Harm       50\n",
      "Absconding      53\n",
      "No_risk       4562\n",
      "dtype: int64\n",
      "\n",
      "SUBCATEGORY DISTRIBUTION:\n",
      "\n",
      "Aggressive:\n",
      "NHC_AV     195\n",
      "NHC_AFO     57\n",
      "NHC_AFP     30\n",
      "dtype: int64\n",
      "\n",
      "Self-Harm:\n",
      "NHC_IA    48\n",
      "NHC_IS     2\n",
      "dtype: int64\n",
      "\n",
      "Absconding:\n",
      "NHC_IF    45\n",
      "NHC_PF     5\n",
      "NHC_F      3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "risk_categories = ['Aggressive', 'Self-Harm', 'Absconding', 'No_risk']\n",
    "print(\"\\nRISK CATEGORIES (Total Counts):\")\n",
    "print(nhc_data[risk_categories].sum())\n",
    "\n",
    "subcategories = {\n",
    "    'Aggressive': ['NHC_AV', 'NHC_AFO', 'NHC_AFP'],\n",
    "    'Self-Harm': ['NHC_IA', 'NHC_IS'],\n",
    "    'Absconding': ['NHC_IF', 'NHC_PF', 'NHC_F']\n",
    "}\n",
    "\n",
    "print(\"\\nSUBCATEGORY DISTRIBUTION:\")\n",
    "for category, cols in subcategories.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(nhc_data[cols].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "400a61b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/88/s1xyjzp127ngj5dnzlh15mb00000gn/T/ipykernel_8582/1789442918.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'M' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  nhc_data.loc[idx, sociodemographic_columns] = found.iloc[0, col_map[cols]].values\n",
      "/var/folders/88/s1xyjzp127ngj5dnzlh15mb00000gn/T/ipykernel_8582/1789442918.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'N' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  nhc_data.loc[idx, sociodemographic_columns] = found.iloc[0, col_map[cols]].values\n",
      "/var/folders/88/s1xyjzp127ngj5dnzlh15mb00000gn/T/ipykernel_8582/1789442918.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'S' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  nhc_data.loc[idx, sociodemographic_columns] = found.iloc[0, col_map[cols]].values\n",
      "/var/folders/88/s1xyjzp127ngj5dnzlh15mb00000gn/T/ipykernel_8582/1789442918.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'N' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  nhc_data.loc[idx, sociodemographic_columns] = found.iloc[0, col_map[cols]].values\n",
      "/var/folders/88/s1xyjzp127ngj5dnzlh15mb00000gn/T/ipykernel_8582/1789442918.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'N' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  nhc_data.loc[idx, sociodemographic_columns] = found.iloc[0, col_map[cols]].values\n",
      "/var/folders/88/s1xyjzp127ngj5dnzlh15mb00000gn/T/ipykernel_8582/1789442918.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'N' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  nhc_data.loc[idx, sociodemographic_columns] = found.iloc[0, col_map[cols]].values\n"
     ]
    }
   ],
   "source": [
    "# Extract Sociodemographic Data from Geslin\n",
    "col_map = {\n",
    "    'P': 15, 'Y': 24, 'AH': 33,\n",
    "    'P_cols': [17, 18, 19, 20, 21, 22],\n",
    "    'Y_cols': [26, 27, 28, 29, 30, 31],\n",
    "    'AH_cols': [35, 36, 37, 38, 39, 40]\n",
    "}\n",
    "sociodemographic_columns = [\n",
    "    'Género', '¿Diagnóstico de esquizofrenia?', '¿Mayor de 35 años?',\n",
    "    'Ingreso involuntario', 'Ingreso por la presencia de riesgo de suicidio',\n",
    "    'Ingreso por riesgo de hacer daño a otros'\n",
    "]\n",
    "nhc_data[sociodemographic_columns] = np.nan\n",
    "\n",
    "for idx, row in nhc_data.iterrows():\n",
    "    nhc = row['NHC']\n",
    "    for col_key, cols in zip(['P', 'Y', 'AH'], ['P_cols', 'Y_cols', 'AH_cols']):\n",
    "        found = geslin_data[geslin_data.iloc[:, col_map[col_key]].astype(str) == nhc]\n",
    "        if not found.empty:\n",
    "            nhc_data.loc[idx, sociodemographic_columns] = found.iloc[0, col_map[cols]].values\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39db8145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder Columns and Export\n",
    "base_cols = ['NHC', 'Fecha', 'Turno']\n",
    "risk_cols = ['Aggressive', 'Self-Harm', 'Absconding', 'No_risk']\n",
    "final_cols = base_cols + sociodemographic_columns + item_acronyms + careload_acronyms + risk_cols\n",
    "nhc_data = nhc_data[final_cols].drop_duplicates(subset=['NHC', 'Fecha', 'Turno'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce98b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export binary dataset\n",
    "nhc_data.to_excel('final_updated_nhc_binary_complete.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5a30ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights\n",
    "item_weights = {\n",
    "    'VIA': 2, 'VIPM': 1, 'AMG': 2, 'ALT': 1, 'CAU': 2, 'AOPAH': 2,\n",
    "    'AMED': 2, 'SRAMF': 1, 'OPCC': 1, 'PAP': 2,\n",
    "    'MVE': 2, 'HVA': 2, 'DCAT': 1, 'PIDT': 1, 'NPMT': 1, 'AFPI': 2,\n",
    "    'DCMSI': 1, 'PII': 1, 'CCA': 1, 'SPS': 1,\n",
    "    'CHP': 2, 'CHO': 2, 'HVT': 1, 'CA': 1, 'DR': 2,\n",
    "    'PH': 1, 'PD-ABVD': 2, 'PAH': 1, 'PCA': 2, 'AC': 2,\n",
    "    'PTCA': 1, 'PCM': 2, 'RAC': 2, 'RAI': 1, 'RCE-PSA': 1, 'RCO': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da9892ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted version\n",
    "df_weighted = nhc_data.copy()\n",
    "for col, weight in item_weights.items():\n",
    "    if col in df_weighted.columns:\n",
    "        df_weighted[col] = df_weighted[col].apply(lambda x: weight if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "272f5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export weighted dataset\n",
    "df_weighted.to_excel(\"final_weighted_dataset.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a97aa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/88/s1xyjzp127ngj5dnzlh15mb00000gn/T/ipykernel_8582/795809389.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  new_dataset = pd.concat([new_dataset, pd.DataFrame([{\n"
     ]
    }
   ],
   "source": [
    "# Complete shifts Geslin\n",
    "\n",
    "# Extract existing shifts from nhc_data\n",
    "existing_turnos = nhc_data[['Fecha', 'Turno']].drop_duplicates()\n",
    "\n",
    "# Prepare the new DataFrame\n",
    "new_dataset = pd.DataFrame(columns=['Fecha', 'Turno', 'Aggressive', 'Self-Harm', 'Absconding'])\n",
    "\n",
    "# Fill the DataFrame with the existing shifts and NHCs\n",
    "for date, turno in existing_turnos.values:\n",
    "    # Filter the NHCs for this date and shift\n",
    "    filtered = nhc_data[(nhc_data['Fecha'] == date) & (nhc_data['Turno'] == turno)]\n",
    "    \n",
    "    # Get the NHCs for each risk category, separated by commas\n",
    "    aggressive_nhcs = ', '.join(filtered[filtered['Aggressive'] == 1]['NHC'].astype(str)) if not filtered[filtered['Aggressive'] == 1].empty else 'NA'\n",
    "    selfharm_nhcs = ', '.join(filtered[filtered['Self-Harm'] == 1]['NHC'].astype(str)) if not filtered[filtered['Self-Harm'] == 1].empty else 'NA'\n",
    "    absconding_nhcs = ', '.join(filtered[filtered['Absconding'] == 1]['NHC'].astype(str)) if not filtered[filtered['Absconding'] == 1].empty else 'NA'\n",
    "    \n",
    "    # Add the row to the new DataFrame\n",
    "    new_dataset = pd.concat([new_dataset, pd.DataFrame([{\n",
    "        'Fecha': date,\n",
    "        'Turno': turno,\n",
    "        'Aggressive': aggressive_nhcs,\n",
    "        'Self-Harm': selfharm_nhcs,\n",
    "        'Absconding': absconding_nhcs\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "# Check geslin_data for missing shifts and add them if they exist\n",
    "for date in existing_turnos['Fecha'].unique():\n",
    "    # Get the shifts for that date in nhc_data\n",
    "    existing_shifts = existing_turnos[existing_turnos['Fecha'] == date]['Turno'].tolist()\n",
    "    \n",
    "    # Get the unique shifts for that date in geslin_data, avoiding NaNs\n",
    "    geslin_shifts = geslin_data[(geslin_data['Fecha'] == date) & (geslin_data['Turno'].notna())]['Turno'].unique()\n",
    "    \n",
    "    # Find which shifts are missing\n",
    "    missing_shifts = list(set(geslin_shifts) - set(existing_shifts))\n",
    "    \n",
    "    # Create rows for the missing shifts\n",
    "    for shift in missing_shifts:\n",
    "        new_dataset = pd.concat([new_dataset, pd.DataFrame([{\n",
    "            'Fecha': date,\n",
    "            'Turno': shift,\n",
    "            'Aggressive': 'NA',\n",
    "            'Self-Harm': 'NA',\n",
    "            'Absconding': 'NA'\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "# Sort by date and shift for clarity\n",
    "new_dataset.sort_values(by=['Fecha', 'Turno'], inplace=True)\n",
    "\n",
    "# Save to Excel with a different name\n",
    "new_dataset.to_excel('turnos_completos_unicos.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97177362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of completion for each shift:\n",
      "M: 97.76%\n",
      "T: 87.96%\n",
      "N: 77.87%\n"
     ]
    }
   ],
   "source": [
    "# Generate a pivot table to see which shifts are present for each date\n",
    "pivot_table = new_dataset.pivot_table(index='Fecha', columns='Turno', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Get the total number of days in the data\n",
    "total_days = len(pivot_table)\n",
    "\n",
    "# Calculate the percentage of completion for each shift\n",
    "percentages = {}\n",
    "for turno in ['M', 'T', 'N']:\n",
    "    completed_days = (pivot_table[turno] > 0).sum()\n",
    "    percentage = (completed_days / total_days) * 100\n",
    "    percentages[turno] = percentage\n",
    "\n",
    "# Display the summary\n",
    "print(\"Percentage of completion for each shift:\")\n",
    "for turno, percentage in percentages.items():\n",
    "    print(f\"{turno}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71df03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add the columns Risk and No_Risk\n",
    "\n",
    "# Replace 'NA' with NaN for easier checks\n",
    "new_dataset.replace('NA', pd.NA, inplace=True)\n",
    "\n",
    "# Define the logic for No_risk and Risk\n",
    "def calculate_risk(row):\n",
    "    # Check if all three are NA\n",
    "    if pd.isna(row['Aggressive']) and pd.isna(row['Self-Harm']) and pd.isna(row['Absconding']):\n",
    "        return 1, 0  # No_risk = 1, Risk = 0\n",
    "    else:\n",
    "        return 0, 1  # No_risk = 0, Risk = 1\n",
    "\n",
    "# Apply the logic to the DataFrame\n",
    "new_dataset[['No_risk', 'Risk']] = new_dataset.apply(calculate_risk, axis=1, result_type='expand')\n",
    "\n",
    "# Replace back the NaNs with 'NA' for clarity\n",
    "new_dataset.fillna('NA', inplace=True)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "new_dataset.to_excel('turnos_completos_unicos_risk.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4fd29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to extract\n",
    "items_columns = [\n",
    "    'VIA', 'VIPM', 'AMG', 'ALT', 'CAU', 'AOPAH', 'AMED', 'SRAMF', 'OPCC',\n",
    "    'PAP', 'MVE', 'HVA', 'DCAT', 'PIDT', 'NPMT', 'AFPI', 'DCMSI', 'PII',\n",
    "    'CCA', 'SPS', 'CHP', 'CHO', 'HVT', 'CA', 'DR', 'PH', 'PD-ABVD', 'PAH',\n",
    "    'PCA', 'AC', 'PTCA', 'PCM', 'RAC', 'RAI', 'RCE-PSA', 'RCO'\n",
    "]\n",
    "\n",
    "# ===========================\n",
    "# Generate the Binary Dataset\n",
    "# ===========================\n",
    "# Take only the turns that exist in nhc_data and the specified items\n",
    "binary_dataset = nhc_data[['NHC', 'Fecha', 'Turno'] + items_columns].drop_duplicates()\n",
    "\n",
    "# Fill missing NHC with 'NA'\n",
    "binary_dataset['NHC'] = binary_dataset['NHC'].fillna('NA')\n",
    "\n",
    "# Fill missing items with 0 (although it should not happen, just in case)\n",
    "binary_dataset[items_columns] = binary_dataset[items_columns].fillna(0)\n",
    "\n",
    "# Drop duplicates if any\n",
    "binary_dataset.drop_duplicates(subset=['Fecha', 'Turno'], keep='first', inplace=True)\n",
    "\n",
    "# Save to Excel\n",
    "binary_dataset.to_excel('binary_items_dataset.xlsx', index=False)\n",
    "\n",
    "# ===========================\n",
    "# Generate the Weighted Dataset\n",
    "# ===========================\n",
    "# Take only the turns that exist in df_weighted and the specified items\n",
    "weighted_dataset = df_weighted[['NHC', 'Fecha', 'Turno'] + items_columns].drop_duplicates()\n",
    "\n",
    "# Fill missing NHC with 'NA'\n",
    "weighted_dataset['NHC'] = weighted_dataset['NHC'].fillna('NA')\n",
    "\n",
    "# Fill missing items with 0 (although it should not happen, just in case)\n",
    "weighted_dataset[items_columns] = weighted_dataset[items_columns].fillna(0)\n",
    "\n",
    "# Drop duplicates if any\n",
    "weighted_dataset.drop_duplicates(subset=['Fecha', 'Turno'], keep='first', inplace=True)\n",
    "\n",
    "# Save to Excel\n",
    "weighted_dataset.to_excel('weighted_items_dataset.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e70d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to extract\n",
    "sociodemographic_columns = [\n",
    "    'NHC', 'Género', '¿Diagnóstico de esquizofrenia?',\n",
    "    '¿Mayor de 35 años?', 'Ingreso involuntario',\n",
    "    'Ingreso por la presencia de riesgo de suicidio',\n",
    "    'Ingreso por riesgo de hacer daño a otros'\n",
    "]\n",
    "\n",
    "# Extract only the necessary columns\n",
    "sociodemographic_data = nhc_data[sociodemographic_columns]\n",
    "\n",
    "# Drop duplicates to have only one row per NHC\n",
    "sociodemographic_data = sociodemographic_data.drop_duplicates(subset=['NHC'], keep='first')\n",
    "\n",
    "# Strip spaces and replace empty strings or spaces with NaN\n",
    "for col in sociodemographic_columns[1:]:\n",
    "    sociodemographic_data[col] = sociodemographic_data[col].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "# Replace empty strings, spaces, and '-' with NaN\n",
    "sociodemographic_data.replace('', pd.NA, inplace=True)\n",
    "sociodemographic_data.replace(' ', pd.NA, inplace=True)\n",
    "sociodemographic_data.replace('-', pd.NA, inplace=True)\n",
    "\n",
    "# Remove rows where all sociodemographic columns are NaN\n",
    "columns_to_check = sociodemographic_columns[1:]  # Skip NHC\n",
    "sociodemographic_data.dropna(subset=columns_to_check, how='all', inplace=True)\n",
    "\n",
    "# Save to Excel\n",
    "sociodemographic_data.to_excel('sociodemographic_data.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
